

---

# Design Project â€” CSE4610

### Implementation and Experimental Analysis of Modern Adversarial Learning and Vision Models

This repository contains the collective work of **Team CSE4610**, where we implemented, analyzed, and partially reproduced several **research papers in adversarial machine learning, vision-language modeling, and diffusion-based robustness**.
The project is organized into distinct modules reflecting each team memberâ€™s research focus and implementation.

---

##  Project Overview

| Member     | Research Papers Implemented                                                                                                                                                                                                                                                                                     | Key Modules                                                     |
| ---------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------- |
| **Aimaan** | 1ï¸. *Large-scale Classification of Fine-Art Paintings: Learning the Right Metric on the Right Feature*  <br>2ï¸. *VLATTACK: Multimodal Adversarial Attacks on Vision-Language Tasks via Pre-trained Models* (partial)                                                                                            | `artwork_classification/`, `vlattack/`                          |
| **Afra**   | 1ï¸. *Explaining and Harnessing Adversarial Examples* (Goodfellow et al., ICLR 2015)  <br>2ï¸. *Retrieval-Augmented Convolutional Neural Networks against Adversarial Examples* (Zhao & Cho, ICLR 2021) using ResNet18                                                                                            | `basic_practice_advAttack/`, `ra-cnn-resnet/`, `ra-cnn-custom/` |
| **Tanjil** | 1ï¸. *Fine-tuning Segment Anything Model (SAM) for Industrial Defect Detection*  <br>2ï¸. *Watermark-Embedded Adversarial Examples for Copyright Protection against Diffusion Models* (Zhu et al., CVPR 2024)  <br>3ï¸. *Retrieval-Augmented Convolutional Neural Networks* (RaCNN) with Foolbox attack evaluation | `fine_tuning_SAM/`, `image_watermarking/`, `racnn_foolbox/`     |

---

##  Research Motivation

Adversarial learning has become a cornerstone for evaluating and enhancing the **robustness, interpretability, and copyright protection** of deep models.
Our goal was to **implement and compare real-world methods** from top-tier research to understand:

* How adversarial perturbations affect different modalities (vision, vision-language).
* How retrieval-based and manifold projection defenses (RaCNN) mitigate such attacks.
* How watermark embedding and diffusion-based protection can secure creative content.
* How pre-trained models like SAM and BLIP behave under fine-tuning or attacks.

---

## ğŸ“‚ Repository Structure

```
DESIGN_PROJECT_CSE4610/
â”‚
â”œâ”€â”€ artwork_classification/       # Aimaan â€“ Fine-Art painting classification & metric learning
â”‚   â”œâ”€â”€ Artwork_Classification.ipynb
â”‚   â”œâ”€â”€ artwork.md
â”‚
â”œâ”€â”€ basic_practice_advAttack/     # Afra â€“ FGSM, meme classifier robustness
â”‚   â”œâ”€â”€ fgsm_meme_classifier.ipynb
â”‚   â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ fine_tuning_SAM/              # Tanjil â€“ Fine-tuning SAM for industrial defects
â”‚   â”œâ”€â”€ fine-tune-sam-for-industrial-defect.ipynb
â”‚   â”œâ”€â”€ Fine_Tune_SAM.md
â”‚
â”œâ”€â”€ image_watermarking/           # Tanjil â€“ Watermark adversarial examples (CVPR 2024)
â”‚   â”œâ”€â”€ watermarking.ipynb
â”‚   â”œâ”€â”€ Watermarking.md
â”‚
â”œâ”€â”€ ra-cnn-custom/                # Afra â€“ RaCNN with vanilla CNN & retrieval augmentation
â”‚   â”œâ”€â”€ ra-cnn-custom.ipynb
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ readme.md
â”‚
â”œâ”€â”€ ra-cnn-resnet/                # Afra â€“ RaCNN with ResNet18 backbone
â”‚   â”œâ”€â”€ RA_CNN_resnet18.ipynb
â”‚   â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ racnn_foolbox/                # Tanjil â€“ RaCNN defense + Foolbox adversarial attacks
â”‚   â”œâ”€â”€ racnn_implement.ipynb
â”‚   â”œâ”€â”€ RACNN_implement.md
â”‚
â”œâ”€â”€ vlattack/                     # Aimaan â€“ Vision-language adversarial attack (VLAttack)
â”‚   â”œâ”€â”€ VLAttack_on_CLIP.ipynb
â”‚   â”œâ”€â”€ vlattack.md
â”‚
â”œâ”€â”€ LICENSE
â””â”€â”€ .gitignore
```

---

## ğŸ”¬ Research Implementations

###  Artwork Classification (Aimaan)

**Paper:** *Large-scale Classification of Fine-Art Paintings: Learning the Right Metric on the Right Feature*
**Objective:** Evaluate CNN + metric learning methods (contrastive / triplet) on art styles and genres.
**Dataset:** [WikiArt Dataset](https://www.wikiart.org/)
**Key Features:**

* Feature extraction using VGG16 / ResNet encoders
* Metric learning with triplet loss
* Cross-artist generalization analysis

###  VLAttack on CLIP (Aimaan)

**Paper:** *VLATTACK: Multimodal Adversarial Attacks on Vision-Language Tasks via Pre-trained Models*
**Objective:** Examine multimodal attacks that disrupt CLIPâ€™s vision-language alignment.
**Implementation:**

* Attack on BLIP/CLIP image-text retrieval
* Perturbations transferred across modalities
* Evaluation on small sample set for feasibility

---

### Adversarial Examples & RaCNN (Afra)

**Papers:**

* *Explaining and Harnessing Adversarial Examples* (Goodfellow et al., 2015)
* *Retrieval-Augmented Convolutional Neural Networks against Adversarial Examples* (Zhao & Cho, 2021)

**Objective:** Study adversarial generation (FGSM, iFGSM) and retrieval-based defense architectures.
**Key Work:**

* Built FGSM-based meme classifier robustness test
* Implemented RaCNN using ResNet18 feature extractor
* Created variant RaCNN with configurable retrieval layers and local mixup

---

###  SAM Fine-tuning for Industrial Defects (Tanjil)

**Objective:** Adapt the *Segment Anything Model (SAM)* for fine-grained defect detection.
**Dataset:** [Few-Shot Industrial Defect Detection (Kaggle)](https://www.kaggle.com/datasets/aryashah2k/few-shot-industrial-defect-detection)
**Key Methods:**

* Fine-tuned SAM ViT-B encoder
* Evaluated IoU improvement for â€œgoodâ€ vs â€œbadâ€ class defects
* Integrated efficient annotation and masking visualization

---

###  Watermark-Embedded Adversarial Examples (Tanjil)

**Paper:** *Watermark-Embedded Adversarial Examples for Copyright Protection against Diffusion Models* (CVPR 2024)
**Objective:** Train a conditional generator to embed artist-specific invisible watermarks in artwork that remain traceable in Stable Diffusion outputs.
**Key Work:**

* Implemented GAN-based perturbation generator
* Used Stable Diffusion v1.5 VAE for latent-space adversarial loss
* Measured watermark persistence via NCC and diffusion response

---

###  RaCNN Defense with Foolbox (Tanjil)

**Paper:** *Retrieval-Augmented Convolutional Neural Networks against Adversarial Examples*
**Objective:** Extend RaCNN defense analysis with Foolbox adversarial attacks.
**Techniques:**

* Implemented FGSM, iFGSM, PGD attacks via Foolbox
* Compared baseline CNN vs RaCNN robustness
* Evaluated performance under Lâˆ and L2 constraints

---

##  Environment Summary

| Library       | Version |
| ------------- | ------- |
| PyTorch       | â‰¥ 2.0   |
| torchvision   | â‰¥ 0.15  |
| diffusers     | â‰¥ 0.29  |
| transformers  | â‰¥ 4.43  |
| pandas        | â‰¥ 2.0   |
| numpy         | â‰¥ 1.25  |
| matplotlib    | â‰¥ 3.7   |
| foolbox       | â‰¥ 3.3   |
| opencv-python | â‰¥ 4.8   |

Each notebook includes its own environment setup cell for reproducibility (Kaggle or Colab compatible).

---

##  License

This repository is distributed for **academic and educational use** under the MIT License.
Please cite the respective original papers when reusing or extending these implementations.

---

##  Contributors

| Name       | ID | Contribution                                          |
| ---------- | -- | ----------------------------------------------------- |
| **Aimaan Ahmed** | 210041204  | Fine-Art classification, VLAttack                     |
| **Afra Anika**   | 210041206 | Adversarial training, RaCNN-ResNet                    |
| **Tanjil Hasan Khan** | 210041246  | SAM fine-tuning, Watermark adversarial, RaCNN defense |

---
